\documentclass[12pt]{article}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[sups]{XCharter}
\usepackage[vvarbb, uprightscript, charter, scaled=1.05]{newtxmath}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage[justification=centering]{caption}
% \usepackage{caption}
% \captionsetup[figure]{skip=1pt}
\usepackage{microtype}
% \usepackage[style=numeric, sorting=none]{biblatex}
% \addbibresource{refs.bib}
% \usepackage{minted}
% \usepackage{fancyhdr}
% \usepackage{gensymb}
% \usepackage{booktabs}
% \usepackage{ntheorem}
% \usepackage{mathtools}
\usepackage{geometry}
% \usepackage{titling}  
\usepackage{indentfirst}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\usepackage{graphicx}
\graphicspath{ {../vis/} }

\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\geometry{a4paper, textwidth=16cm, textheight=24cm}

\newcommand{\mpl}[2]{
    \begin{figure}[!h]
        \includegraphics[width=0.98\textwidth]{#1}
        \centering
        \caption{#2}
        \label{fig:#1}
     \end{figure}
}

\title{Отчет по заданию №3:\break Композиции алгоритмов для решения \break задачи регрессии}
\author{Васильев Руслан \and{ВМК МГУ, 317 группа}}

\begin{document}

\maketitle
\tableofcontents
\newpage
% \setcounter{secnumdepth}{0}
\section{Введение}
В заключительном практическом задании предлагается реализовать композиции алгоритмов машинного обучения и провести эксперименты, а также спроектировать веб-сервис для взаимодействия с моделью. Весь проект доступен в репозитории\footnote{\url{https://github.com/artnitolog/mmf_prac_2020_task_3}}. Данный отчет иллюстрирует результаты экспериментов с моделями на датасете данных о продажи недвижимости.

\section{Постановка задачи}

Итак, рассматривается задача регрессии с метрикой качества RMSE:
\begin{equation*}
    \operatorname{RMSE} = \sqrt{\frac{\sum_{i=1}^{N} (y_i - \hat{y}_i)^2}{N}},
\end{equation*}
где $N$~--- размер выборки, $y_i$~--- истинное значение целевой переменной на $i$-м объекте, $\hat{y}_i$~--- предсказанное.

Для решения реализованы две модели, представляющие собой ансамбли решающих деревьев: случайный лес и градиентный бустинг. Исследование алгоритмов включает в себя измерение функции ошибки и времени работы при варьировании гиперпараметров (порядок экспериментов соответствует стандартной настройке данных моделей).

\section{Эксперименты}
Кроме ошибки (RMSE), проводится измерение времени работы. Причем на графиках изображено усредненное время: каждая модель запускается трижды.

\subsection{Предобработка данных}
Исходные данные о недвижимости были разделены на обучение (80\%) и контроль (20\%, она же валидационная выборка). И здесь сразу учитывается особенность задачи. Хотя в задании отсутствует описание признаков и целевой переменной, можно с уверенностью предположить, что столбец \verb|date| связан со временем поступления данных (даты имеют небольшой диапазон 2014--2015, монотонно возрастают, дублируются, следуют сразу за \verb|ID|, а столбцы \verb|build_year| и \verb|renovation_year| с ними не связаны). По этой причине было бы некорректно перемешать выборку перед разделением на обучение и контроль~--- из-за утечки такая стратегия может дать ложную оценку качества моделей и привести к неправильным выводам. В качестве валидационной выборки берутся последние 20\% данных, соответствующие хронологическому порядку по столбцу \verb|date|.

\subsection{Случайный лес}
\subsubsection{Количество деревьев}
Количество деревьев в случайном лесе регулирует число алгоритмов, по которому проводится ансамблирование (усреднение). На \autoref{fig:RF_n_estimators_RMSE} можно видеть, что с ростом числа деревьев ошибка практически монотонно убывает на обучении выборке. Тем не менее на контроле по достижении оптимального числа базовых алгоритмов функционал затем немного увеличивается (переобучение), а затем выходит на асимптоту. Для нашей задачи нам оказалось достаточно взять 250 деревьев.

\mpl{RF_n_estimators_RMSE}{Зависимость RMSE от количества деревьев в случайном лесе}

Что касается времени работы, то понятно, что оно должно линейно зависеть от числа деревьев в лесе. Для более честной оценки обучим с нуля несколько моделей с разным количеством деревьев, результаты приведены на \autoref{fig:RF_n_estimators_time}. И действительно, время обучения растет линейно.
\mpl{RF_n_estimators_time}{Зависимость времени обучения случайного леса от числа деревьев}

\subsubsection{Размерность подвыборки признаков для одного дерева}
В случайном лесе данный параметр может сильно повлиять на качество предсказания. В задаче регрессии обычно берут либо все признаки, либо треть от их числа. Именно последний вариант оказался выигрышным в нашей задаче (\autoref{fig:RF_max_features_RMSE}). На обучении, как и следовало ожидать, ошибка монотонно убывает с увеличением числа признаков.
\mpl{RF_max_features_RMSE}{Зависимость RMSE от максимального числа признаков (для одного дерева в случайном лесе)}

Что касается времени работы (\autoref{fig:RF_max_features_time}) — с ростом размерности подвыборки признаков время растет~--- относительно линейно.
\mpl{RF_max_features_time}{Зависимость времени обучения случайного леса от максимального числа признаков (для одного дерева)}

\subsubsection{Глубина дерева}
Случайный лес обычно состоит из глубоких переобученных деревьев. И наша задача не стала исключением. \autoref{fig:RF_max_depth_RMSE} показывает, что лучшее качество регрессии достигается на деревьях без ограничений на глубину.  
\mpl{RF_max_depth_RMSE}{Зависимость RMSE от глубины одного дерева в случайном лесе}

Возможно, при большем объеме выборки или более высокой размерности признакового пространства ограничение на глубину имело бы смысл с точки зрения времени обучения. Но \autoref{fig:RF_max_depth_time} показывает, что в нашей задаче, ограничив глубину, на достаточном уровне (примерно $>13$), особого выигрыша в производительности не будет.
\mpl{RF_max_depth_time}{Зависимость времени обучения случайного леса от глубины одного дерева}

\subsection{Градиентный бустинг}
\subsubsection{Количество деревьев и темп обучения}
В отличие от случайного леса, в градиентном бустинге базовые алгоритмы не являются независимыми~--- каждый следующий исправляет ошибки предыдущих. Поэтому при настройке гиперпараметров количество деревьев не подбирается отдельно, а рассматривается в паре с темпом обучения. Рассмотрим зависимость RMSE на обучающей и контрольной выборках (\autoref{fig:GBM_lr_RMSE}).

\mpl{GBM_lr_RMSE}{Зависимость RMSE от числа деревьев и темпа обучения в градиентном бустинге}

На обучении (\autoref{fig:GBM_lr_RMSE}) ошибка с ростом числа деревьев монотонно стремится к нулю. Но на тестовой выборке в градиентном бустинге обычно монотонной зависимости нет. При высоком темпе обучения, как видим, качество на валидации действительно начинает ухудшаться с некоторого момента. Но при <<умеренном>> значении \verb|learning_rate| ошибка, пусть крайне медленно, но продолжает уменьшаться даже после тысячи деревьев. Такое поведение связано с использованием классической реализации бустинга и особенностями задачи. 

Если на обучающей выборке ошибка уменьшается с ростом темпа обучения, то на валидации зависимость обратная (на большом числе деревьев). Но для дальнейшего проведения экспериментов особого смысла в 2000 деревьев и темпе 0.01 нет~--- разница в качестве незначительная. Поэтому дальше для следующих пунктов оставим 400 деревьев с \verb|learning_rate|~=~0.1. Время работы в нашей реализации по-прежнему линейно растет с числом базовых алгоритмов, не не зависит от темпа обучения\footnote{Хотя время могло бы уменьшаться с ростом темпа обучения, если добавить критерий останова при отсутствии улучшения качества на валидации.}. Результат подверждается графиком \autoref{fig:GBM_lr_time}.

\mpl{GBM_lr_time}{Зависимость времени обучения градиентного бустинга от числа деревьев и темпа}

Итак, \autoref{fig:GBM_lr_time} показывает, что в нашей реализации бустинга время обучения растет линейно с числом деревьев и не зависит от темпа обучения. Но если сравнить со случайным лесом (\autoref{fig:RF_n_estimators_time}), то при бустинге обучение происходит быстрее. Но почему? Причина заключается в разных подходах к настройке глубины для бустинга и леса.

\subsubsection{Глубина дерева}
\mpl{GBM_max_depth_RMSE}{Зависимость RMSE от глубины одного дерева в градиентном бустинге}
В отличие от случайного леса, в градиентном бустинге обычно используются неглубокие деревья. Причину можно проиллюстрировать \autoref{fig:GBM_max_depth_RMSE}. На обучающей бустинг глубоких деревьев очень быстро переобучается, при отсутствии ограничений потери почти нулевые. И такое обучение приводит к некачественной работе на тестовой (в нашем случае валидационной) выборке. Тем не менее проблем с подбором нужной глубины у нас не возникло: видно, что 5~--- оптимальное значение.
Время работы алгоритма (\autoref{fig:GBM_max_depth_time}) заметно увеличивается с ростом глубины, хотя, не считая полное отсутствие ограничений, зависимость на рассмотренных значениях можно считать линейной.
\mpl{GBM_max_depth_time}{Зависимость времени обучения градиентного бустинга от глубины одного дерева}

\subsubsection{Размерность подвыборки признаков для дерева}
В данном гиперпараметре снова проявляется отличие от случайного леса. На \autoref{fig:RF_max_features_RMSE} виден оптимум (на контроле) и стационарное значение RMSE поведение с дальнейшим ростом числа признаков. На \autoref{fig:GBM_max_features_RMSE} ситуация нестабильная. Но на большом числе признаков ошибка больше, чем на <<среднем>>. 
\mpl{GBM_max_features_RMSE}{Зависимость RMSE от максимального числа признаков (для одного дерева в градиентном бустинге)}
Значение 12 кажется оптимальным (по валидации), но для более тщательного отбора следовало бы рассмотреть графики он числа деревьев.
\mpl{GBM_max_features_time}{Зависимость времени обучения градиентного бустинга от максимального числа признаков (для одного дерева)}

Время работы можем видеть на \autoref{fig:GBM_max_features_time}: здесь достаточно точно прослеживается линейная зависимость.

\section{Заключение}
\mpl{conclusion}{Ошибки итоговых моделей}

По итогам экспериментов можно обучить алгоритмы с лучшими гиперпараметрами (из исследованных, все конкретные значения, обучение и построение графиков указаны в jupyter-ноутбуке, расположенном в репозитории).
Для нашей задачи градиентный бустинг, на контроле $\operatorname{RMSE}<118$, оказался предпочтительней случайного леса с $\operatorname{RMSE}>130$.

В небольшой серии экспериментов мы сравнили влияние гиперпараметров на случайный лес и бустинг над деревьями~--- отличия проявляются и в диапазоне оптимальных значениях, и в стратегии настройки, и в значимости для качества модели. 
\end{document}