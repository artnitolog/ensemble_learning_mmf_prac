\documentclass[12pt]{article}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[sups]{XCharter}
\usepackage[vvarbb, uprightscript, charter, scaled=1.05]{newtxmath}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage[justification=centering]{caption}
% \usepackage{caption}
% \captionsetup[figure]{skip=1pt}
\usepackage{microtype}
% \usepackage[style=numeric, sorting=none]{biblatex}
% \addbibresource{refs.bib}
% \usepackage{minted}
% \usepackage{fancyhdr}
% \usepackage{gensymb}
% \usepackage{booktabs}
% \usepackage{ntheorem}
% \usepackage{mathtools}
\usepackage{geometry}
% \usepackage{titling}  
\usepackage{indentfirst}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\usepackage{graphicx}
\graphicspath{ {../vis/} }

\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\geometry{a4paper, textwidth=16cm, textheight=24cm}

\newcommand{\mpl}[2]{
    \begin{figure}[!h]
        \includegraphics[width=0.98\textwidth]{#1}
        \centering
        \caption{#2}
        \label{fig:#1}
     \end{figure}
}

\title{Отчет по заданию №3:\break Композиции алгоритмов для решения \break задачи регрессии}
\author{Васильев Руслан \and{ВМК МГУ, 317 группа}}

\begin{document}

\maketitle
\tableofcontents
\newpage
% \setcounter{secnumdepth}{0}
\section{Введение}
В заключительном практическом задании предлагается реализовать композиции алгоритмов машинного обучения и провести эксперименты, а также спроектировать веб-сервис для взаимодействия с моделью. Весь проект доступен в репозитории\footnote{\url{https://github.com/artnitolog/mmf_prac_2020_task_3}}. Данный отчет иллюстрирует результаты экспериментов с моделями на датасете данных о продажи недвижимости.

\section{Постановка задачи}

Итак, рассматривается задача регрессии с метрикой качества RMSE:
\begin{equation*}
    \operatorname{RMSE} = \sqrt{\frac{\sum_{i=1}^{N} (y_i - \hat{y}_i)}{N}},
\end{equation*}
где $N$~--- размер выборки, $y_i$~--- истинное значение целевой переменной на $i$-м объекте, $\hat{y}_i$~--- предсказанное.

Для решения реализованы две модели, представляющие собой ансамбли решающих деревьев: случайный лес и градиентный бустинг. Исследование алгоритмов включает в себя измерение функции ошибки и времени работы при варьировании гиперпараметров (порядок экспериментов соответствует стандартной настройке данных моделей).

\section{Эксперименты}
Кроме ошибки (RMSE), проводится измерение времени работы. На графиках приведено усредненное время: каждая модель запускается трижды.

\subsection{Предобработка данных}
Исходные данные о недвижимости были разделены на обучение (80\%) и контроль (20\%, она же валидационная выборка). И здесь сразу учитывается особенность задачи. Хотя в задании отсутствует описание признаков и целевой переменной, можно с уверенностью предположить, что столбец \verb|date| связан со временем поступления данных (даты имеют небольшой диапазон 2014--2015, монотонно возрастают, дублируются, следуют сразу за \verb|ID|, а столбцы \verb|build_year| и \verb|renovation_year| с ними не связаны). По этой причине было бы некорректно перемешать выборку перед разделением на обучение и контроль~--- из-за утечки такая стратегия может дать ложную оценку качества моделей и привести к неправильным выводам. В качестве валидационной выборки берутся последние 20\% данных, соответствующие хронологическому порядку по столбцу \verb|date|.

\subsection{Случайный лес}
\subsubsection{Количество деревьев}
Количество деревьев в случайном лесе регулирует число алгоритмов, по которому проводится ансамблирование (усреднение). На \autoref{fig:RF_n_estimators_RMSE} можно видеть, что с ростом числа деревьев ошибка практически монотонно убывает на обучении выборке. Тем не менее на контроле по достижении оптимального числа базовых алгоритмов функционал затем немного увеличивается (переобучение), а затем выходит на асимптоту. Для нашей задачи нам оказалось достаточно взять 250 деревьев.

\mpl{RF_n_estimators_RMSE}{Зависимость RMSE от количества деревьев в случайном лесе}

Что касается времени работы, то понятно, что оно должно линейно зависеть от числа деревьев в лесе. Для более честной оценки обучим с нуля несколько моделей с разным количеством деревьев, результаты приведены на \autoref{fig:RF_n_estimators_time}. И действительно, время обучения растет линейно.
\mpl{RF_n_estimators_time}{Зависимость времени обучения случайного леса от числа деревьев}

\subsubsection{Размерность подвыборки признаков для одного дерева}
В случайном лесе данный параметр может сильно повлиять на качество предсказания. В задаче регрессии обычно берут либо все признаки, либо треть от их числа. Именно последний вариант оказался выигрышным в нашей задаче (\autoref{fig:RF_max_features_RMSE}). На обучении, как и следовало ожидать, ошибка монотонно убывает с увеличением числа признаков.
\mpl{RF_max_features_RMSE}{Зависимость RMSE от максимального числа признаков (для одного дерева в случайном лесе)}

% \mpl{RF_max_features_time}{Зависимость времени обучения случайного леся от максимального числа признаков (для одного дерева)}

\subsubsection{Глубина дерева}

\subsection{Градиентный бустинг}
\subsubsection{Количество деревьев и темп обучения}
В отличие от случайного леса, в градиентном бустинге базовые алгоритмы не являются независимыми~--- каждый следующий исправляет ошибки предыдущих. Поэтому при настройке гиперпараметров количество деревьев не подбирается отдельно, а рассматривается в паре с темпом обучения. Рассмотрим зависимость RMSE на обучающей и контрольной выборках (\autoref{fig:GBM_lr_RMSE}).

\mpl{GBM_lr_RMSE}{Зависимость RMSE от числа деревьев и темпа обучения в градиентном бустинге}

На обучении (\autoref{fig:GBM_lr_RMSE}) ошибка с ростом числа деревьев монотонно стремится к нулю. Но на валидации и тесте в градиентном бустинге обычно монотонной зависимости нет. При высоком темпе обучения, как видим, качество действительно начинает ухудшаться с некоторого момента. Но при <<умеренном>> значении \verb|learning_rate| ошибка, пусть крайне медленно, но продолжает уменьшаться даже после тысячи деревьев. Такое поведение связано с использованием классической реализации бустинга и особенностями задачи. 

Если на обучающей выборке ошибка уменьшается с ростом темпа обучения, то на валидации зависимость обратная (на большом числе деревьев). Но для дальнейшего проведения экспериментов особого смысла в 2000 деревьев и темпе 0.01 нет~--- разница в качестве незначительная. Поэтому дальше рассмотрим 500 деревьев с \verb|learning_rate|~=~0.1. Время работы в нашей реализации по-прежнему линейно растет с числом базовых алгоритмов, не не зависит от темпа обучения\footnote{Хотя время могло бы уменьшаться с ростом темпа обучения, если добавить критерий останова при отсутствии улучшения качества на валидации.}. Подтвердим это экспериментом с обучением разных моделей с нуля, но на уменьшенном числе деревьев.

\mpl{GBM_lr_time}{Зависимость времени обучения градиентного бустинга от числа деревьев и темпа}

Итак, \autoref{fig:GBM_lr_time} показывает, что в нашей реализации бустинга время обучения растет линейно с числом деревьев и не зависит от темпа обучения. Но если сравнить со случайным лесом (\autoref{fig:RF_n_estimators_time}), то при бустинге обучение происходит быстрее. Но почему? Причина заключается в разных подходах к настройке глубины для бустинга и леса.

\subsubsection{Глубина дерева}
\subsubsection{Размерность подвыборки признаков для дерева}
\section{Заключение}
\end{document}